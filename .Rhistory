cm
logr.model
source('~/Downloads/3-sol.R', echo=TRUE)
source('~/Downloads/3-sol.R', echo=TRUE)
train.err
source('~/Downloads/3-sol.R', echo=TRUE)
library('tm')
library("rjson")
library("KernSmooth")
library('SnowballC')
library('wordcloud')
setwd("/Users/zdai/Desktop/UT/random_acts_of_pizza_time/")
install.packages("tm", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
install.packages("rjson", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
install.packages("SnowballC", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
install.packages("wordcloud", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library('tm')
library("rjson")
library("KernSmooth")
library('SnowballC')
library('wordcloud')
setwd("/Users/zdai/Desktop/UT/random_acts_of_pizza_time/")
install.packages("NLP", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library(rjson)
library(plyr)
library(NLP)
options(java.parameters = "-Xmx4g")
library(openNLP)
library(tm)
library(topicmodels)
library(slam)
library(caret)
library(wordcloud)
library(skmeans)
options(stringsAsFactors = F)
install.packages("skmeans", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
install.packages("topicmodels", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
install.packages("openNLP", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
options(stringsAsFactors = F)
train_df = fromJSON(file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.json') # Convert JSON object to R object
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
names(train_data) = names(train_df[[1]]) # TR = TRAIN
names(train_data) = names(train[[1]]) # TR = TRAIN
train_data
train_data = data.frame(matrix(unlist(train_df), byrow = T, nrow = length(train_df)))
library(rjson)
library(plyr)
library(NLP)
library(openNLP)
library(tm)
library(topicmodels)
library(slam)
library(caret)
library(wordcloud)
library(skmeans)
options(java.parameters = "-Xmx4g")
options(stringsAsFactors = F)
train_df = fromJSON(file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.json') # Convert JSON object to R object
train_df = lapply(train_df, lapply,
function(x) ifelse(is.null(x), NA, x))
train_df = lapply(train_df, lapply, lapply,
function(x) ifelse(is.null(x), NA, x))
train_data = data.frame(matrix(unlist(train_df), byrow = T, nrow = length(train_df)))
# Set object train_data a name
names(train_data) = names(train[[1]]) # TR = TRAIN
# Export and save to a R object
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
names(train_data) = names(train_df[[1]]) # TR = TRAIN
# Export and save to a R object
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
load("/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata")
request = tolower(request)
request = tolower(request)
print request
request
req = paste(train_data$request_title, train_data$request_text_edit_aware)
req
load("~/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata")
load("~/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata")
req
req
req = gsub('[^[:alpha:]]',' ', req)
req
req = tolower(req)
req
req = gsub('[^[:alpha:]]','', req)
req
load("/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata")
req = paste(train_data$request_title, train_data$request_text_edit_aware)
req
req = tolower(req)
req = gsub('[^[:alpha:]]',' ', req)
train_data$req = req
req
train_data$req = req
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
corpus = tm_map(corpus, remove_space)
c = Corpus(VectorSource(req))
corpus = tm_map(orig, removeWords, orig(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
corpus = tm_map(orig, removeWords, c(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
c = Corpus(VectorSource(req))
corpus = tm_map(orig, removeWords, c(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
orig = Corpus(VectorSource(req))
corpus = tm_map(orig, removeWords, c(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
corpus
corpus = tm_map(corpus, remove_space)
c = Corpus(VectorSource(req))
corpus = tm_map(c, removeWords, c(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
corpus
corpus = tm_map(corpus, remove_space)
corpus = tm_map(c, removeWords, c(stopwords('SMART'),pizza','request','pizzas','requests'))
corpus = tm_map(c, removeWords, c(stopwords('SMART'),'pizza','request','pizzas','requests'))
corpus = tm_map(corpus, remove_space)
orig = Corpus(VectorSource(req))
corpus = tm_map(orig, removeWords, c(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
corpus = tm_map(corpus, remove_space)
save(corpus_tagged, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train_tagged.Rdata')
corpus_tagged = lapply(corpus,POS_tagger)
POS_tagger = function(w){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sentence_annotator = Maxent_Sent_Token_Annotator()
word_annotator = Maxent_Word_Token_Annotator()
pos_tagger = Maxent_POS_Tag_Annotator()
arr = annotate(x, list(sentence_annotator,word_annotator))
arr = annotate(x, pos_tagger, arr)
word = subset(arr, typr == 'word')
return(unlist(word$features))
}
corpus_tagged = lapply(corpus,POS_tagger)
corpus_tagged = lapply(corpus,POS_tagger)
pos_tagger = Maxent_POS_Tag_Annotator()
POS_tagger = function(w){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sentence_annotator = Maxent_Sent_Token_Annotator()
word_annotator = Maxent_Word_Token_Annotator()
arr = annotate(x, list(sentence_annotator,word_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
arr = annotate(x, pos_tagger, arr)
word = subset(arr, typr == 'word')
return(unlist(word$features))
}
corpus_tagged = lapply(corpus,POS_tagger)
POS_tagger = function(x){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sentence_annotator = Maxent_Sent_Token_Annotator()
word_annotator = Maxent_Word_Token_Annotator()
arr = annotate(x, list(sentence_annotator,word_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
arr = annotate(x, pos_tagger, arr)
word = subset(arr, typr == 'word')
return(unlist(word$features))
}
corpus_tagged = lapply(corpus,POS_tagger)
POS_tagger = function(x){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sentence_annotator = Maxent_Sent_Token_Annotator()
word_annotator = Maxent_Word_Token_Annotator()
arr = annotate(x, list(sentence_annotator,word_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
arr = annotate(x, pos_tagger, arr)
word = subset(arr, type == 'word')
return(unlist(word$features))
}
corpus_tagged = lapply(corpus,POS_tagger)
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sentence_annotator = Maxent_Sent_Token_Annotator()
word_annotator = Maxent_Word_Token_Annotator()
a = annotate(x, list(sentence_annotator,word_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
a = annotate(x, list(sentence_annotator,word_annotator))
sentence_annotator = Maxent_Sent_Token_Annotator()
sentence_annotator
list(sentence_annotator,word_annotator)
annotate(x, list(sentence_annotator,word_annotator))
library("qdap")
install.packages("qdap", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
req
a = annotate(x, list(sentence_annotator,word_annotator))
sentence_annotator = qdap.Maxent_Sent_Token_Annotator()
sentence_annotator = qdap.Maxent_Sent_Token_Annotator()
?Maxent_Sent_Token_Annotator()
sentence_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator(
word_token_annotator <- Maxent_Word_Token_Annota
exit
word_token_annotator <- Maxent_Word_Token_Annotator()
a <- annotate(s, list(sentence_token_annotator,word_token_annotator))
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a <- annotate(s, list(sent_token_annotator, word_token_annotator))
packageDescription("openNLP")
with(sentSplit(tm_corpus2df(current.corpus), "text"), df2tm_corpus(tot, text))
word_token_annotator
sent_token_annotator <- Maxent_Sent_Token_Annotator()
sent_token_annotator
word_token_annotator <- Maxent_Word_Token_Annotator()
word_token_annotator
a <- annotate(s, list(sent_token_annotator, word_token_annotator))
a <- annotate(s, sent_token_annotator)
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ",
"nonexecutive director Nov. 29.\n",
"Mr. Vinken is chairman of Elsevier N.V., ",
"the Dutch publishing group."),
collapse = "")
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
sent_token_annotator
a1 <- annotate(s, sent_token_annotator)
a1
library(plyr)
library(rjson)
library(plyr)
library(NLP)
library(openNLP)
library(tm)
library(topicmodels)
library(slam)
library(caret)
library(wordcloud)
library(skmeans)
options(java.parameters = "-Xmx4g")
options(stringsAsFactors = F)
# Step 1: Tranfer JSON formated data to Rdata
train_df = fromJSON(file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.json') # Convert JSON object to R object
train_df = lapply(train_df, lapply,
function(x) ifelse(is.null(x), NA, x))
train_df = lapply(train_df, lapply, lapply,
function(x) ifelse(is.null(x), NA, x))
train_data = data.frame(matrix(unlist(train_df), byrow = T, nrow = length(train_df)))
# Set object train_data a name
names(train_data) = names(train_df[[1]]) # TR = TRAIN
# Export and save to a R object
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
#Step2:  Data preparation
load("/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata")
remove_space = function(w) {
w = gsub('^ +','',w)
w = gsub(' +$','',w)
w = gsub(' +', ' ',w) # Insert space bewteen each word
}
# Concatenate vector
req = paste(train_data$request_title, train_data$request_text_edit_aware)
#Unified case
req = tolower(req)
# Ignore "noise" words, emotion icon, specific symbols
req = gsub('[^[:alpha:]]',' ', req)
req = remove_space(req)
train_data$req = req
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
# Remove stop word and generate corpus
orig = Corpus(VectorSource(req))
corpus = tm_map(orig, removeWords, c(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
corpus = tm_map(corpus, remove_space)
#Tokenization
POS_tagger = function(s){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sent_token_annotator <- Maxent_Sent_Token_Annotator()
#sent_token_annotator
word_token_annotator <- Maxent_Word_Token_Annotator()
#word_token_annotator
a <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
a = annotate(s, pos_tagger, a)
word = subset(a, type == 'word')
return(unlist(word$features))
}
corpus_tagged = lapply(corpus,POS_tagger)
corpus_tagged = lapply(corpus,POS_tagger)
library(rjson)
library(plyr)
library(NLP)
library(openNLP)
library(tm)
library(topicmodels)
library(slam)
library(caret)
library(wordcloud)
library(skmeans)
options(java.parameters = "-Xmx4g")
options(stringsAsFactors = F)
# Step 1: Tranfer JSON formated data to Rdata
train_df = fromJSON(file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.json') # Convert JSON object to R object
train_df = lapply(train_df, lapply,
function(x) ifelse(is.null(x), NA, x))
train_df = lapply(train_df, lapply, lapply,
function(x) ifelse(is.null(x), NA, x))
train_data = data.frame(matrix(unlist(train_df), byrow = T, nrow = length(train_df)))
# Set object train_data a name
names(train_data) = names(train_df[[1]]) # TR = TRAIN
# Export and save to a R object
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
#Step2:  Data preparation
load("/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata")
remove_space = function(w) {
w = gsub('^ +','',w)
w = gsub(' +$','',w)
w = gsub(' +', ' ',w) # Insert space bewteen each word
}
# Concatenate vector
req = paste(train_data$request_title, train_data$request_text_edit_aware)
#Unified case
req = tolower(req)
# Ignore "noise" words, emotion icon, specific symbols
req = gsub('[^[:alpha:]]',' ', req)
req = remove_space(req)
train_data$req = req
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
# Remove stop word and generate corpus
orig = Corpus(VectorSource(req))
#corpus = tm_map(orig, removeWords, c(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
corpus = tm_map(orig, removeWords, c(stopwords('SMART'), 'pizza','pizzas','request','requests'))
corpus = tm_map(corpus, remove_space)
#Tokenization
POS_tagger = function(s){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sent_token_annotator <- Maxent_Sent_Token_Annotator()
#sent_token_annotator
word_token_annotator <- Maxent_Word_Token_Annotator()
#word_token_annotator
a <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
a = annotate(s, pos_tagger, a)
word = subset(a, type == 'word')
return(unlist(word$features))
}
corpus_tagged = lapply(corpus,POS_tagger)
save(corpus_tagged, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train_tagged.Rdata')
POS_tagger = function(s, a){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sent_token_annotator <- Maxent_Sent_Token_Annotator()
#sent_token_annotator
word_token_annotator <- Maxent_Word_Token_Annotator()
#word_token_annotator
a <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
a = annotate(s, pos_tagger, a)
word = subset(a, type == 'word')
return(unlist(word$features))
}
corpus_tagged = lapply(corpus,POS_tagger)
?lapply
POS_tagger = function(s, a = Annotation()){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sent_token_annotator <- Maxent_Sent_Token_Annotator()
#sent_token_annotator
word_token_annotator <- Maxent_Word_Token_Annotator()
#word_token_annotator
a <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
a = annotate(s, pos_tagger, a)
word = subset(a, type == 'word')
return(unlist(word$features))
}
corpus_tagged = lapply(corpus,POS_tagger)
library(rjson)
library(plyr)
library(NLP)
options(java.parameters = "-Xmx4g")
library(openNLP)
library(tm)
library(topicmodels)
library(slam)
library(caret)
library(wordcloud)
library(skmeans)
options(stringsAsFactors = F)
train = fromJSON(file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.json')
train = lapply(train, lapply, function(x) ifelse(is.null(x), NA, x))
train = lapply(train, lapply, lapply, function(x) ifelse(is.null(x), NA, x))
train_df = data.frame(matrix(unlist(train), byrow = T, nrow = length(train)))
names(train_df) = names(train[[1]])
save(train_df, file = 'train.RData')
load(file = 'train.RData')
#clean the request fields (incl. request title)
req = paste(train_df$request_title, train_df$request_text_edit_aware)
req = tolower(req)
req = gsub('[^[:alpha:]]', ' ', req)
rm_space = function(x) {
x = gsub('^ +', '', x)
x = gsub(' +$', '', x)
x = gsub(' +', ' ', x)
}
req = rm_space(req)
train_df$req = req
save(train_df, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/tr.RDATA')
#create corpus and remove stopwords
c = Corpus(VectorSource(req))
c_clean = tm_map(c, removeWords, c(stopwords('SMART'), 'pizza', 'pizzas', 'request', 'requests'))
c_clean = tm_map(c_clean, rm_space)
#function to keep only nouns (per the paper)
pos_tag = function(x) {
gc() #clean garbage to free up memory space (otherwise an error may be thrown out reporting memory shortage)
sent_token_annotator = Maxent_Sent_Token_Annotator()
word_token_annotator = Maxent_Word_Token_Annotator()
a = annotate(x, list(sent_token_annotator, word_token_annotator))
pos_tag_annotator = Maxent_POS_Tag_Annotator()
a = annotate(x, pos_tag_annotator, a)
w = subset(a, type == 'word')
return(unlist(w$features))
}
c_t = lapply(c_clean, pos_tag)
library(rjson)
library(plyr)
library(NLP)
library(openNLP)
library(tm)
library(topicmodels)
library(slam)
library(caret)
library(wordcloud)
library(skmeans)
options(java.parameters = "-Xmx4g")
options(stringsAsFactors = F)
# Step 1: Tranfer JSON formated data to Rdata
train_df <- fromJSON(file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.json') # Convert JSON object to R object
train_df <- lapply(train_df, lapply,
function(x) ifelse(is.null(x), NA, x))
train_df <- lapply(train_df, lapply, lapply,
function(x) ifelse(is.null(x), NA, x))
train_data < data.frame(matrix(unlist(train_df), byrow = T, nrow = length(train_df)))
# Set object train_data a name
names(train_data) <- names(train_df[[1]]) # TR = TRAIN
# Export and save to a R object
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
#Step2:  Data preparation
load("/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata")
remove_space = function(w) {
w = gsub('^ +','',w)
w = gsub(' +$','',w)
w = gsub(' +', ' ',w) # Insert space bewteen each word
}
# Concatenate vector
req <- paste(train_data$request_title, train_data$request_text_edit_aware)
#Unified case
req <- tolower(req)
# Ignore "noise" words, emotion icon, specific symbols
req = gsub('[^[:alpha:]]',' ', req)
req <= remove_space(req)
train_data$req <- req
save(train_data, file = '/Users/zdai/Desktop/UT/random_acts_of_pizza_time/PredictPizza_models/train.Rdata')
# Remove stop word and generate corpus
orig <- Corpus(VectorSource(req))
#corpus = tm_map(orig, removeWords, c(stopwords('SMART'), 'the','on','of','pizza','request','pizzas','requests'))
corpus <- tm_map(orig, removeWords, c(stopwords('SMART'), 'pizza','pizzas','request','requests'))
corpus <- tm_map(corpus, remove_space)
#Tokenization
POS_tagger = function(s){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sent_token_annotator <- Maxent_Sent_Token_Annotator()
#sent_token_annotator
word_token_annotator <- Maxent_Word_Token_Annotator()
#word_token_annotator
a <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
a <- annotate(s, pos_tagger, a)
word <- subset(a, type == 'word')
return(unlist(word$features))
}
corpus_tagged <- lapply(corpus,POS_tagger)
?as.data.frame.default(x[[i]], optional = TRUE)
?as.data.frame.default()
?lapply(corpus
,POS_tagger)
lapply(corpus,POS_tagger)
POS_tagger = function(text, lang = "en"){
#Keep warning message away, and release memory
gc()
# Using openNLP maxent sentence detector to generate annotator for each sentence and word
sent_token_annotator <- Maxent_Sent_Token_Annotator()
#sent_token_annotator
word_token_annotator <- Maxent_Word_Token_Annotator()
#word_token_annotator
a <- annotate(text, list(sent_token_annotator, word_token_annotator))
pos_tagger = Maxent_POS_Tag_Annotator()
a <- annotate(s, pos_tagger, a)
word <- subset(a, type == 'word')
return(unlist(word$features))
}
corpus_tagged <- lapply(current.corpus,POS_tagger)
sent_detect <- function(text, language) {
# Function to compute sentence annotations using the Apache OpenNLP Maxent sentence detector employing the default model for language 'en'.
sentence_token_annotator <- Maxent_Sent_Token_Annotator(language)
# Convert text to class String from package NLP
text <- as.String(text)
# Sentence boundaries in text
sentence.boundaries <- annotate(text, sentence_token_annotator)
# Extract sentences
sentences <- text[sentence.boundaries]
# return sentences
return(sentences)
}
tiny <- sent_detect("hi i am Apple", language = "en")
